[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análise de dados de Metagenômica",
    "section": "",
    "text": "Processamento e Análise de Dados de Metagenoma\nNeste repositório está o material para o curso Análise de dados de Metagenômica, organizado pelo Prof. Rodrigo Dalmolin, do Centro Multiusuário de Bioinformática da UFRN.\nO curso é dividido em 5 módulos:\n\nControle de qualidade e pré-processamento\nMontagem\nClassificação taxonômica\nAnotação funcional\nAnálises downstream:\n\nCálculos de diversidade e visualizações\nVisualizações da anotação funcional",
    "crumbs": [
      "Processamento e Análise de Dados de Metagenoma"
    ]
  },
  {
    "objectID": "content/01_setup.html",
    "href": "content/01_setup.html",
    "title": "1  Organizando Ambiente para Análise",
    "section": "",
    "text": "Primeiro, é necessário criar e ativar um ambiente Conda que contenha todas as ferramentas necessárias para os próximos passos. Caso ainda não tenha o Conda instalado, siga as instruções neste link.\n$ conda env create -f medusaPipeline.yml\n$ conda activate medusaPipeline\nA estrutura de diretórios a seguir ajudará na organização dos arquivos baixados, arquivos intermediários e seus outputs:\n$ mkdir -p ./Pipeline/{result,data/{merged,assembled,collapsed,removal/{index,reference},raw,trimmed},alignment/{db,index},taxonomic/db,functional/db}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organizando Ambiente para Análise</span>"
    ]
  },
  {
    "objectID": "content/02_qc.html",
    "href": "content/02_qc.html",
    "title": "2  Controle de Qualidade e Pré-processamento",
    "section": "",
    "text": "2.1 Baixar Amostras\nMude para o diretório Pipeline/data e baixe os arquivos de exemplo em pares (pair-end):",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controle de Qualidade e Pré-processamento</span>"
    ]
  },
  {
    "objectID": "content/02_qc.html#baixar-amostras",
    "href": "content/02_qc.html#baixar-amostras",
    "title": "2  Controle de Qualidade e Pré-processamento",
    "section": "",
    "text": "$ cd Pipeline/data\n$ fasterq-dump SRR579292 -e 8\n\n\n\n\n\n\nNota\n\n\n\nO argumento -e no comando fasterq-dump especifica o número de threads que serão utilizadas. Adapte este argumento conforme o desempenho do seu sistema.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controle de Qualidade e Pré-processamento</span>"
    ]
  },
  {
    "objectID": "content/02_qc.html#controle-de-qualidade-com-fastqc-e-multiqc",
    "href": "content/02_qc.html#controle-de-qualidade-com-fastqc-e-multiqc",
    "title": "2  Controle de Qualidade e Pré-processamento",
    "section": "2.2 Controle de Qualidade com FastQC e MultiQC",
    "text": "2.2 Controle de Qualidade com FastQC e MultiQC\n\n2.2.1 Estrutura de Arquivos .fastq\nO formato .fastq contém as sequências de DNA geradas no sequenciamento, junto com informações sobre a qualidade de cada nucleotídeo. A estrutura de um arquivo .fastq segue um padrão repetitivo a cada fragmento sequenciado:\n\nLinha 1: Contém o identificador único da leitura, que pode incluir informações sobre o sequenciador e a amostra (começa com o símbolo @).\nLinha 2: A sequência de nucleotídeos resultante do sequenciamento.\nLinha 3: Opcionalmente usada para anotações ou descrições adicionais (começa com um símbolo +).\nLinha 4: A qualidade de cada base na sequência, representada pelo Phred Score, que reflete a confiabilidade da leitura.\n\nA imagem abaixo exemplifica essa estrutura:\n\n\n\n\n\n\nEm Roxo: Identificador do sequenciamento.\nEm Laranja: Sequências adaptadoras (caso ainda não tenham sido removidas), que podem variar dependendo da plataforma de sequenciamento.\nEm Azul: O fragmento de DNA sequenciado, conhecido como “Insert Size”.\nEm Verde: Phred Score, que indica a qualidade de cada base na sequência.\n\n\n\n2.2.2 Ferramentas de Controle de Qualidade\nPara garantir que as sequências obtidas sejam de boa qualidade e adequadas para análise, utilizamos ferramentas como o FastQC e o MultiQC.\n\nFastQC: Avalia a qualidade de cada arquivo de sequenciamento individualmente, gerando relatórios com informações sobre a qualidade das bases, conteúdo GC, presença de adaptadores e outros aspectos que podem impactar a análise.\nMultiQC: Agrega os relatórios gerados pelo FastQC (ou outras ferramentas), criando um único relatório consolidado que facilita a visualização e interpretação dos dados de múltiplas amostras.\n\n\n\n2.2.3 Gerando Relatórios de Qualidade\nPara gerar relatórios de controle de qualidade com o FastQC para cada uma das amostras baixadas, use o seguinte comando:\n# Gerando relatórios de qualidade para cada amostra\nfor sample in $(ls Pipeline/data/*.fastq.gz); \ndo\nfastqc $sample -o Pipeline/data\ndone\nGere o relatório consolidado contendo todas as amostras a partir do output do FastQC:\n# Consolidando relatórios com MultiQC \n$ multiqc Pipeline/data\n\n\n\n\n\n\nInterpretando os resultados do MultiQC\n\n\n\nVerifique os gráficos de qualidade, presença de contaminantes e distribuições de qualidade das bases. Ajustes podem ser necessários para garantir a integridade dos dados para as etapas seguintes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controle de Qualidade e Pré-processamento</span>"
    ]
  },
  {
    "objectID": "content/03_montagem.html",
    "href": "content/03_montagem.html",
    "title": "3  Montagem",
    "section": "",
    "text": "3.1 Contexto\nAs reads, ou leituras, fragmentos de sequências gerados pelo processo de sequenciamento, podem ser re-organizadas e mescladas em sequências mais longas e contíguas, ou contigs. Esse processo é denominado de Montagem.\nPara se realizar montagem, você pode utilizar uma referência, como o genoma referência de um organismo, que servirá como base para organizar as contigs. No entanto, no contexto metagenômico, a modalidade de montagem geralmente realizada é a montagem livre de referência, ou montagem de novo.\nComo montador de novo, utilizaremos o MEGAHIT.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Montagem</span>"
    ]
  },
  {
    "objectID": "content/03_montagem.html#realizando-a-montagem",
    "href": "content/03_montagem.html#realizando-a-montagem",
    "title": "3  Montagem",
    "section": "3.2 Realizando a montagem",
    "text": "3.2 Realizando a montagem\nVamos montar as leituras pós-descontaminação usando o MEGAHIT:\nmegahit -1 ../removal/unaligned_1.fastq -2\n../removal/unaligned_2.fastq -o SRR579292 -t 8 \nO arquivo de saída SRR579292.contigs.fa contém as sequências contíguas correspondentes às leituras usadas.\n\n\n\n\n\n\nNota\n\n\n\nOs passos seguintes, como a classificação taxonômica, podem se utilizar tanto das leituras quanto dos contigs. Iremos utilizar as leituras por motivos de didática, mas a maioria das ferramentas utilizadas para a análise de metagenômica podem fazer uso tanto de reads quanto de contigs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Montagem</span>"
    ]
  },
  {
    "objectID": "content/04_taxonomia.html",
    "href": "content/04_taxonomia.html",
    "title": "4  Classificação Taxonômica",
    "section": "",
    "text": "4.1 Contexto",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classificação Taxonômica</span>"
    ]
  },
  {
    "objectID": "content/04_taxonomia.html#realizando-a-classificação-taxonômica",
    "href": "content/04_taxonomia.html#realizando-a-classificação-taxonômica",
    "title": "4  Classificação Taxonômica",
    "section": "4.2 Realizando a classificação taxonômica",
    "text": "4.2 Realizando a classificação taxonômica\nMude o diretório atual para “Pipeline/taxonomic/db” e baixe os seguintes arquivos:\n$ wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz\n$ wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.gz\nEsses são os bancos de dados que utilizaremos para executar o Kaiju\nExtraia e descompacte os arquivos necessários:\n$ tar -xf taxdump.tar.gz nodes.dmp names.dmp\n$ pigz -d prot.accession2taxid.gz -p 8\nConstrua um índice Kaiju:\n$ kaiju-convertNR -t nodes.dmp -g prot.accession2taxid -e ~/miniconda3/envs/medusaPipeline/bin/kaiju-excluded-accessions.txt -a -o kaijuNR.fasta -i ../../alignment/db/nr\n$ kaiju-mkbwt -n 8 -a ACDEFGHIKLMNPQRSTVWY -o kaijuNR kaijuNR.fasta\n$ kaiju-mkfmi kaijuNR\nNota: Na chamada kaiju-convertNR, por padrão, são incluídas apenas sequências de Archaea, Bactérias e Vírus do NCBI-nr. Este comportamento pode ser alterado com o argumento -l, passando um arquivo de entrada como ~/miniconda3/envs/medusaPipeline/bin/kaiju-taxonlistEuk.tsv. Este argumento utiliza apenas sequências com ancestrais listados no arquivo.\nMude o diretório atual para “Pipeline/taxonomic” e execute a classificação taxonômica:\n$ kaiju -t db/nodes.dmp -f db/kaijuNR.fmi -i ../data/removal/unaligned_1.fastq -j ../data/removal/unaligned_2.fastq -o ../result/SRR579292_kaiju.out -z 8\nAdicione os nomes dos táxons ao output:\n$ kaiju-addTaxonNames -t db/nodes.dmp -n db/names.dmp -r superkingdom,phylum,class,order,family,genus,species -i ../result/SRR579292_kaiju.out -o ../result/SRR579292_kaiju.names\nGere os gráficos Krona:\n$ kaiju2krona -t db/nodes.dmp -n db/names.dmp -i ../result/SRR579292_kaiju.out -o ../result/SRR579292_kaiju2krona.out\n$ ktImportText -o ../result/SRR579292_krona.html ../result/SRR579292_kaiju2krona.out",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classificação Taxonômica</span>"
    ]
  },
  {
    "objectID": "content/05_anotacao.html",
    "href": "content/05_anotacao.html",
    "title": "5  Anotação Funcional",
    "section": "",
    "text": "5.1 annotate\nPara nosso primeiro exemplo, vamos utilizar a ferramenta annotate, que irá transferir identificadores de um resultado de alinhamento para identificadores funcionais, nesse caso termos do gene ontology.\nMas, para fazer isso, primeiro precisamos alinhar nosso dado a uma referência!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Anotação Funcional</span>"
    ]
  },
  {
    "objectID": "content/05_anotacao.html#annotate",
    "href": "content/05_anotacao.html#annotate",
    "title": "5  Anotação Funcional",
    "section": "",
    "text": "5.1.1 Alinhamento\n\n\n5.1.2 Executando o annotate\n\nVamos primeiro filtrar as colunas do nosso arquivo de referência para as que contém identificadores do GenBank e do Gene Ontology:\n\nawk -F \"\\\\t\" '{if((\\$7!=\"\") && (\\$18!=\"\")){print \\$18\"\\\\t\"\\$7}}' idmapping_selected.tab &gt; genbank2GO.txt\n\nE fazer o mesmo para termos um arquivo que traduz identificadores RefSeq para Gene Ontology:\n\nawk -F \"\\\\t\" '{if((\\$4!=\"\") && (\\$7!=\"\")){print \\$4\"\\\\t\"\\$7}}' idmapping_selected.tab &gt; refseq2GO.txt\n\nPodemos então executar um script R que irá limpar os dois arquivos, preparando-os para o formato do annotate:\n\n\n\n\n\n\n\nAviso\n\n\n\nO script requer bastante memória para lidar com o arquivo de mapeamento do UniProt, podendo requerir até 80GB de memória RAM para um dicionário típico.\n\n\ncreateDictionary.R \\\n        NR2GO.txt \\\n        genbank2GO.txt \\\n        refseq2GO.txt \\\n        4\n\nEm sequência, criamos o banco de dados do annotate:\n\nannotate createdb NR2GO.txt NR2GO 0 1 -d db\n\nE executamos o annotate para anotar cada query do alinhamento para seu identificador funcional:\n\nannotate idmapping ../alignment/all_matches.m8 ../result/SRR579292_functional_GO.txt NR2GO -l 1 -d db\n\n\n\n\n\n\nNota\n\n\n\nO argumento -l determina qual o comprimento mínimo de um alinhamento para ele ser considerado.\n\n\n\nAo explorar o arquivo de resultado do annotate podemos ver que ele possui o seguinte formato: …",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Anotação Funcional</span>"
    ]
  },
  {
    "objectID": "content/06_analise_downstream.html",
    "href": "content/06_analise_downstream.html",
    "title": "6  Análises Downstream",
    "section": "",
    "text": "6.1 Cálculos de Diversidade",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Análises Downstream</span>"
    ]
  },
  {
    "objectID": "content/06_analise_downstream.html#extraindo-informação-funcional",
    "href": "content/06_analise_downstream.html#extraindo-informação-funcional",
    "title": "6  Análises Downstream",
    "section": "6.2 Extraindo informação funcional",
    "text": "6.2 Extraindo informação funcional",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Análises Downstream</span>"
    ]
  }
]