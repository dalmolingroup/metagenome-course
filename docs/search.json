[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análise de dados de Metagenômica",
    "section": "",
    "text": "Processamento e Análise de Dados de Metagenoma\nNeste repositório está o material para o curso Análise de dados de Metagenômica, organizado pelo Prof. Rodrigo Dalmolin, do Centro Multiusuário de Bioinformática da UFRN.\nO curso é dividido em 5 módulos:\n\nControle de qualidade e pré-processamento\nMontagem\nClassificação taxonômica\nAnotação funcional\nAnálises downstream:\n\nCálculos de diversidade e visualizações\nVisualizações da anotação funcional",
    "crumbs": [
      "Processamento e Análise de Dados de Metagenoma"
    ]
  },
  {
    "objectID": "content/01_setup.html",
    "href": "content/01_setup.html",
    "title": "1  Organizando Ambiente para Análise",
    "section": "",
    "text": "Primeiro, é necessário criar e ativar um ambiente Conda que contenha todas as ferramentas necessárias para os próximos passos. Caso ainda não tenha o Conda instalado, siga as instruções neste link.\nVá até este link para adquirir o arquivo environment.yml.\nconda env create -f environment.yml\nconda activate medusaPipeline\nA estrutura de diretórios a seguir ajudará na organização dos arquivos baixados, arquivos intermediários e seus outputs:\nmkdir -p ./Pipeline/{result,data/{merged,assembled,collapsed,removal/{index,reference},raw,trimmed},alignment/{db,index},taxonomic/db,functional/db}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organizando Ambiente para Análise</span>"
    ]
  },
  {
    "objectID": "content/02_qc.html",
    "href": "content/02_qc.html",
    "title": "2  Controle de Qualidade e Pré-processamento",
    "section": "",
    "text": "2.1 Baixar Amostras\nMude para o diretório Pipeline/data e baixe os arquivos de exemplo em pares (pair-end):",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controle de Qualidade e Pré-processamento</span>"
    ]
  },
  {
    "objectID": "content/02_qc.html#baixar-amostras",
    "href": "content/02_qc.html#baixar-amostras",
    "title": "2  Controle de Qualidade e Pré-processamento",
    "section": "",
    "text": "$ cd Pipeline/data\n$ fasterq-dump SRR579292 -e 8\n\n\n\n\n\n\nNota\n\n\n\nO argumento -e no comando fasterq-dump especifica o número de threads que serão utilizadas. Adapte este argumento conforme o desempenho do seu sistema.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controle de Qualidade e Pré-processamento</span>"
    ]
  },
  {
    "objectID": "content/02_qc.html#controle-de-qualidade-com-fastqc-e-multiqc",
    "href": "content/02_qc.html#controle-de-qualidade-com-fastqc-e-multiqc",
    "title": "2  Controle de Qualidade e Pré-processamento",
    "section": "2.2 Controle de Qualidade com FastQC e MultiQC",
    "text": "2.2 Controle de Qualidade com FastQC e MultiQC\n\n2.2.1 Estrutura de Arquivos .fastq\nO formato .fastq contém as sequências de DNA geradas no sequenciamento, junto com informações sobre a qualidade de cada nucleotídeo. A estrutura de um arquivo .fastq segue um padrão repetitivo a cada fragmento sequenciado:\n\nLinha 1: Contém o identificador único da leitura, que pode incluir informações sobre o sequenciador e a amostra (começa com o símbolo @).\nLinha 2: A sequência de nucleotídeos resultante do sequenciamento.\nLinha 3: Opcionalmente usada para anotações ou descrições adicionais (começa com um símbolo +).\nLinha 4: A qualidade de cada base na sequência, representada pelo Phred Score, que reflete a confiabilidade da leitura.\n\nA imagem abaixo exemplifica essa estrutura:\n\n\n\n\n\n\nEm Roxo: Identificador do sequenciamento.\nEm Laranja: Sequências adaptadoras (caso ainda não tenham sido removidas), que podem variar dependendo da plataforma de sequenciamento.\nEm Azul: O fragmento de DNA sequenciado, conhecido como “Insert Size”.\nEm Verde: Phred Score, que indica a qualidade de cada base na sequência.\n\n\n\n2.2.2 Ferramentas de Controle de Qualidade\nPara garantir que as sequências obtidas sejam de boa qualidade e adequadas para análise, utilizamos ferramentas como o FastQC e o MultiQC.\n\nFastQC: Avalia a qualidade de cada arquivo de sequenciamento individualmente, gerando relatórios com informações sobre a qualidade das bases, conteúdo GC, presença de adaptadores e outros aspectos que podem impactar a análise.\nMultiQC: Agrega os relatórios gerados pelo FastQC (ou outras ferramentas), criando um único relatório consolidado que facilita a visualização e interpretação dos dados de múltiplas amostras.\n\n\n\n2.2.3 Gerando Relatórios de Qualidade\nPara gerar relatórios de controle de qualidade com o FastQC para cada uma das amostras baixadas, use o seguinte comando:\n# Gerando relatórios de qualidade para cada amostra\nfor sample in $(ls Pipeline/data/*.fastq.gz); \ndo\nfastqc $sample -o Pipeline/data\ndone\nGere o relatório consolidado contendo todas as amostras a partir do output do FastQC:\n# Consolidando relatórios com MultiQC \n$ multiqc Pipeline/data\n\n\n\n\n\n\nInterpretando os resultados do MultiQC\n\n\n\nVerifique os gráficos de qualidade, presença de contaminantes e distribuições de qualidade das bases. Ajustes podem ser necessários para garantir a integridade dos dados para as etapas seguintes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controle de Qualidade e Pré-processamento</span>"
    ]
  },
  {
    "objectID": "content/02_qc.html#pré-processamento-das-leituras",
    "href": "content/02_qc.html#pré-processamento-das-leituras",
    "title": "2  Controle de Qualidade e Pré-processamento",
    "section": "2.3 Pré-processamento das leituras",
    "text": "2.3 Pré-processamento das leituras\n\n2.3.1 Contexto\nComo vimos na seção anterior, nosso arquivo pode ter sequências de baixa qualidade, sejam estas sequências adaptadoras ou simplesmente erros no procedimento do sequenciamento. Portanto, devemos remover essas sequências de nossas análises posteriores, a fim de não permitir que interfiram com ruído na informação biológica real.\nPara isso, obteremos uma amostra de sequenciamento (que será utilizada em todas as seções posteriores) e a processaremos com ferramentas de controle de qualidade, além de alinhar a amostra, de origem na microbiota humana, contra o genoma referência do hospedeiro, garantido que tenhamos apenas sequências advindas da microbiota.\n\n\n2.3.2 Obtendo os dados\nMude o diretório atual para \"Protocol/data\" e baixe os dados de exemplo:\n$ prefetch SRR579292 -X 20G  \n\n\n\n\n\n\nNota\n\n\n\nNo comando prefetch, o argumento -X especifica o tamanho máximo do arquivo a ser baixado (adapte esse argumento se necessário).\n\n\nObtenha as leituras paired-end:\n$ fasterq-dump SRR579292 -e 8  \n\n\n\n\n\n\nNota\n\n\n\nNo comando fasterq-dump, o argumento -e especifica o número de threads a serem usadas (adapte esse argumento se necessário).\n\n\n\n\n2.3.3 Removendo sequências de baixa qualidade\nRemova sequências de baixa qualidade e adaptadores:\n$ fastp -i SRR579292_1.fastq -I SRR579292_2.fastq -o trimmed/SRR579292_1_trim.fastq -O trimmed/SRR579292_2_trim.fastq -q 20 -w 8 --detect_adapter_for_pe -h trimmed/report.html -j trimmed/fastp.json  \n\n\n\n\n\n\nNota\n\n\n\nNo comando fastp, o argumento -q define o limiar da pontuação de qualidade Phred, e -w especifica o número de threads a serem usadas (adapte esses argumentos se necessário).\n\n\nUm genoma de referência de Homo sapiens é necessário para remover as sequências do hospedeiro desses dados. Mude o diretório atual para \"Pipeline/data/removal/reference\" e baixe um genoma de referência do Ensembl.\n\n\n2.3.4 Remoção de Sequências do Hospedeiro\nBaixe o genoma de referência de Homo sapiens do Ensembl:\n$ wget ftp://ftp.ensembl.org/pub/release-102/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz  \n$ pigz -d Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz -p 8  \n\n\n\n\n\n\nNota\n\n\n\nO software pigz é uma implementação paralela do gzip. O argumento -p especifica o número de threads a serem usadas.\n\n\nConstrua um índice Bowtie2:\n$ bowtie2-build Homo_sapiens.GRCh38.dna.primary_assembly.fa ../index/host --threads 8  \nVolte para \"Pipeline/data\" e alinhe as sequências contra a referência:\n$ bowtie2 -x removal/index/host -1 trimmed/SRR579292_1_trim.fastq -2 trimmed/SRR579292_2_trim.fastq -S removal/all.sam -p 8  \nExtraia as leituras não alinhadas:\n$ samtools view -bS removal/all.sam &gt; removal/all.bam  \n$ samtools view -b -f 12 -F 256 removal/all.bam &gt; removal/unaligned.bam  \n$ samtools sort -n removal/unaligned.bam -o removal/unaligned_sorted.bam  \n$ samtools bam2fq removal/unaligned_sorted.bam &gt; removal/unaligned.fastq  \n$ cat removal/unaligned.fastq | grep '^@.*/1$' -A 3 --no-group-separator &gt; removal/unaligned_1.fastq  \n$ cat removal/unaligned.fastq | grep '^@.*/2$' -A 3 --no-group-separator &gt; removal/unaligned_2.fastq  \n\n\n\n\n\n\nNota\n\n\n\nNo comando SAMtools, os argumentos -f e -F utilizam flags para especificar, respectivamente, quais alinhamentos devem ser extraídos e quais não devem ser extraídos. Consulte esta documentação para saber mais sobre as flags disponíveis no SAMtools.\n\n\nUna as leituras paired-end:\n$ fastp -i removal/unaligned_1.fastq -I removal/unaligned_2.fastq -o trimmed/unmerged_1.fastq -O trimmed/unmerged_2.fastq -q 20 -w 8 --detect_adapter_for_pe -h trimmed/report2.html -j trimmed/fastp2.json -m --merged_out merged/SRR579292_merged.fastq  \nRemova sequências duplicadas:\n$ fastx_collapser -i merged/SRR579292_merged.fastq -o collapsed/SRR579292.fasta",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controle de Qualidade e Pré-processamento</span>"
    ]
  },
  {
    "objectID": "content/03_montagem.html",
    "href": "content/03_montagem.html",
    "title": "3  Montagem",
    "section": "",
    "text": "3.1 Contexto\nAs reads, ou leituras, fragmentos de sequências gerados pelo processo de sequenciamento, podem ser re-organizadas e mescladas em sequências mais longas e contíguas, ou contigs. Esse processo é denominado de Montagem.\nPara se realizar montagem, você pode utilizar uma referência, como o genoma referência de um organismo, que servirá como base para organizar as contigs. No entanto, no contexto metagenômico, a modalidade de montagem geralmente realizada é a montagem livre de referência, ou montagem de novo.\nComo montador de novo, utilizaremos o MEGAHIT.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Montagem</span>"
    ]
  },
  {
    "objectID": "content/03_montagem.html#realizando-a-montagem",
    "href": "content/03_montagem.html#realizando-a-montagem",
    "title": "3  Montagem",
    "section": "3.2 Realizando a montagem",
    "text": "3.2 Realizando a montagem\nVamos montar as leituras pós-descontaminação usando o MEGAHIT:\nmegahit -1 ../removal/unaligned_1.fastq -2\n../removal/unaligned_2.fastq -o SRR579292 -t 8 \nO arquivo de saída SRR579292.contigs.fa contém as sequências contíguas correspondentes às leituras usadas.\n\n\n\n\n\n\nNota\n\n\n\nOs passos seguintes, como a classificação taxonômica, podem se utilizar tanto das leituras quanto dos contigs. Iremos utilizar as leituras por motivos de didática, mas a maioria das ferramentas utilizadas para a análise de metagenômica podem fazer uso tanto de reads quanto de contigs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Montagem</span>"
    ]
  },
  {
    "objectID": "content/04_taxonomia.html",
    "href": "content/04_taxonomia.html",
    "title": "4  Classificação Taxonômica",
    "section": "",
    "text": "4.1 Contexto\nO passo de classificação taxonômica trata de classificar as sequências mistas, sejam leituras ou contíguas, de uma amostra metagenômica em táxons claramente definidos, e tipicamente acompanha um perfil de abundância desses táxons.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classificação Taxonômica</span>"
    ]
  },
  {
    "objectID": "content/04_taxonomia.html#realizando-a-classificação-taxonômica",
    "href": "content/04_taxonomia.html#realizando-a-classificação-taxonômica",
    "title": "4  Classificação Taxonômica",
    "section": "4.3 Realizando a classificação taxonômica",
    "text": "4.3 Realizando a classificação taxonômica\nMude o diretório atual para “Pipeline/taxonomic” e execute a classificação taxonômica:\nkaiju -t db/nodes.dmp -f db/kaijuNR.fmi -i ../data/removal/unaligned_1.fastq -j ../data/removal/unaligned_2.fastq -o ../result/SRR579292_kaiju.out -z 8\nAdicione os nomes dos táxons ao output:\nkaiju-addTaxonNames -t db/nodes.dmp -n db/names.dmp -r superkingdom,phylum,class,order,family,genus,species -i ../result/SRR579292_kaiju.out -o ../result/SRR579292_kaiju.names\nGere os gráficos Krona:\nkaiju2krona -t db/nodes.dmp -n db/names.dmp -i ../result/SRR579292_kaiju.out -o ../result/SRR579292_kaiju2krona.out\nktImportText -o ../result/SRR579292_krona.html ../result/SRR579292_kaiju2krona.out",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classificação Taxonômica</span>"
    ]
  },
  {
    "objectID": "content/05_anotacao.html",
    "href": "content/05_anotacao.html",
    "title": "5  Anotação Funcional",
    "section": "",
    "text": "5.1 annotate\nPara nosso primeiro exemplo, vamos utilizar a ferramenta annotate, que irá transferir identificadores de um resultado de alinhamento para identificadores funcionais, nesse caso termos do gene ontology.\nMas, para fazer isso, primeiro precisamos alinhar nosso dado a uma referência!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Anotação Funcional</span>"
    ]
  },
  {
    "objectID": "content/05_anotacao.html#annotate",
    "href": "content/05_anotacao.html#annotate",
    "title": "5  Anotação Funcional",
    "section": "",
    "text": "5.1.1 Alinhamento\n\n5.1.1.1 Realizando o alinhamento\nMude o diretório atual para \"Pipeline/alignment\" e alinhe as leituras contra o banco de dados de proteínas de referência:\n$ touch unaligned.fasta  \n$ diamond blastx -d index/nr -q ../data/collapsed/SRR579292.fasta -o matches.m8 --top 3 --un unaligned.fasta  \n\n\n\n\n\n\nNota\n\n\n\nNo comando DIAMOND, o argumento --un especifica o arquivo usado para gravar as sequências não alinhadas, e --top 3 reporta alinhamentos dentro dessa porcentagem da melhor pontuação de alinhamento.\nO DIAMOND pode usar muita memória e espaço temporário em disco. Portanto, o programa pode falhar ao esgotar um desses recursos. O argumento -b especifica o tamanho do bloco e -c o número de partes para processamento do índice. O uso total de memória pode ser estimado aproximadamente como 2(b+9b/c) GB. Em um servidor com alta memória, defina -c 1.\n\n\nRealize um alinhamento mais sensível usando as sequências não alinhadas:\n$ touch unaligned2.fasta  \n$ diamond blastx -d index/nr -q unaligned.fasta -o matches2.m8 --more-sensitive --top 3 --un unaligned2.fasta  \n\n\n\n\n\n\nNota\n\n\n\nO modo padrão sensível é projetado para leituras curtas (~100 pb), encontrando hits com &gt;60% de identidade. Para sequências mais longas, os modos sensíveis são recomendados. O DIAMOND é muito mais eficiente para arquivos grandes (&gt;1 milhão de leituras).\n\n\nVerifique o número de consultas que apresentam hits com pelo menos 80% de identidade em matches2.m8:\n$ awk '{ if ($3&gt;=80) { print } }' matches2.m8 &gt; check.m8  \n$ awk '{a[$1]++}END{for (i in a) sum+=1; print sum}' check.m8  \n\n\n5.1.1.2 Concatenar os resultados do alinhamento:\n$ cat matches.m8 matches2.m8 &gt; all_matches.m8  \n\n\n\n5.1.2 Executando o annotate\n\nVamos primeiro filtrar as colunas do nosso arquivo de referência para as que contém identificadores do GenBank e do Gene Ontology:\n\nawk -F \"\\\\t\" '{if((\\$7!=\"\") && (\\$18!=\"\")){print \\$18\"\\\\t\"\\$7}}' idmapping_selected.tab &gt; genbank2GO.txt\n\nE fazer o mesmo para termos um arquivo que traduz identificadores RefSeq para Gene Ontology:\n\nawk -F \"\\\\t\" '{if((\\$4!=\"\") && (\\$7!=\"\")){print \\$4\"\\\\t\"\\$7}}' idmapping_selected.tab &gt; refseq2GO.txt\n\nPodemos então executar um script R que irá limpar os dois arquivos, preparando-os para o formato do annotate:\n\n\n\n\n\n\n\nAviso\n\n\n\nO script requer bastante memória para lidar com o arquivo de mapeamento do UniProt, podendo requerir até 80GB de memória RAM para um dicionário típico.\n\n\ncreateDictionary.R \\\n        NR2GO.txt \\\n        genbank2GO.txt \\\n        refseq2GO.txt \\\n        4\n\nEm sequência, criamos o banco de dados do annotate:\n\nannotate createdb NR2GO.txt NR2GO 0 1 -d db\n\nE executamos o annotate para anotar cada query do alinhamento para seu identificador funcional:\n\nannotate idmapping ../alignment/all_matches.m8 ../result/SRR579292_functional_GO.txt NR2GO -l 1 -d db\n\n\n\n\n\n\nNota\n\n\n\nO argumento -l determina qual o comprimento mínimo de um alinhamento para ele ser considerado.\n\n\n\nAo explorar o arquivo de resultado do annotate podemos ver que ele possui o seguinte formato:\n\nQuery   Annotation\n342-2   GO:0005829; GO:0008720; GO:0047964; GO:0030267; GO:0016618; GO:0051287\n1560-1  GO:0005829; GO:0005886; GO:0003854; GO:0102294; GO:0070403; GO:0016616; GO:0004769; GO:0030283; GO:0016042; GO:0006694; GO:0008202\n3588-1  GO:0000775; GO:0005737; GO:0005829; GO:0090443; GO:0110085; GO:0044732; GO:0005634; GO:0000159; GO:0019888; GO:0061509; GO:0030952; GO:1990813; GO:0031030; GO:0006470\n5204-1  GO:0016491; GO:0008202\n7527-1  Unknown\n9319-1  GO:0042802; GO:0030170; GO:0008483; GO:0006520; GO:0009058\n9922-1  GO:0000775; GO:0005737; GO:0005829; GO:0090443; GO:0110085; GO:0044732; GO:0005634; GO:0000159; GO:0019888; GO:0061509; GO:0030952; GO:1990813; GO:0031030; GO:0006470\n10306-1 Unknown\n12743-1 GO:0005524; GO:0140658; GO:0004386; GO:0016787\nPara cada busca (“Query”) do seu alinhamento original, temos uma ou mais anotações em termos do Gene Ontology (“Annotation”)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Anotação Funcional</span>"
    ]
  },
  {
    "objectID": "content/05_anotacao.html#eggnog-mapper",
    "href": "content/05_anotacao.html#eggnog-mapper",
    "title": "5  Anotação Funcional",
    "section": "5.2 eggNOG-mapper",
    "text": "5.2 eggNOG-mapper\nO eggNOG-mapper é uma ferramenta que realiza anotação funcional de sequências contíguas. Ele usa informação de ortologia e filogenia para transferir identificadores funcionais às sequências. O eggNOG-mapper pode ser executado através de uma interface web mas aqui ilustraremos seu uso na linha de comando.\nPara isso, vamos utilizar as sequências contíguas que montamos anteriormente, na Seção 3.\n\nO eggNOG-mapper pode ser executado da seguinte maneira:\n\nemapper.py \\\n        -m diamond \\\n        --itype metagenome \\\n        -i SRR579292.contigs.fa \\\n        -o eggnog_results/SRR579292 \\\n        --cpu 6\nAs opções que utilizamos foram:\n\n-m: Sinaliza o “modo” de execução do eggnog, neste caso a ferramenta que será utilizada para realizar a busca por sequências. No nosso caso utilizamos o DIAMOND, ferramenta que já conhecemos na Sessão 5.1.1.\n--itype: Sinaliza o tipo de entrada (input) que damos a ferramenta, no nosso caso sequência contíguas originadas de um metagenoma.\n-i: Determina a entrada para a ferramenta.\n-o: Determina para onde deve ser escrita a saída da ferramenta.\n--cpu: O número de núcleos de processamento a serem utilizados pela ferramenta.\n\nUma vez que executamos o eggNOG-mapper, teremos um resultado no seguinte formato (eggnog_results/SRR579292.emapper.annotations):\n#query  seed_ortholog   evalue  score   eggNOG_OGs      max_annot_lvl   COG_category    Description     Preferred_name  GOs     EC      KEGG_ko KEGG_Pathway    KEGG_Module     KEGG_Reaction   KEGG_rclass     BRITE      KEGG_TC CAZy    BiGG_Reaction   PFAMs\nk141_94572      552396.HMPREF0863_01053 4.43e-45        159.0   COG0595@1|root,COG0595@2|Bacteria,1TQ9G@1239|Firmicutes,3VP2V@526524|Erysipelotrichia   526524|Erysipelotrichia S       Psort location Cytoplasmic, score  -       -       -       ko:K12574       ko03018,map03018        -       -       -       ko00000,ko00001,ko01000,ko03019 -       -       -       Lactamase_B,Lactamase_B_2,RMMBL\nk141_47286      1236514.BAKL01000034_gene2896   3.35e-79        254.0   COG5009@1|root,COG5009@2|Bacteria,4NECJ@976|Bacteroidetes,2FNAU@200643|Bacteroidia,4AKYH@815|Bacteroidaceae     976|Bacteroidetes       M COG5009 Membrane carboxypeptidase penicillin-binding protein     mrcA    -       2.4.1.129,3.4.16.4      ko:K05366       ko00550,ko01100,ko01501,map00550,map01100,map01501      -       -       -       ko00000,ko00001,ko01000,ko01003,ko01011    -       GT51    -       Transgly,Transpeptidase\nk141_141858     158189.SpiBuddy_1606    8.08e-76        239.0   COG1593@1|root,COG1593@2|Bacteria,2J6XN@203691|Spirochaetes     203691|Spirochaetes     G       transporter, DctM subunit       -       -       - --       -       -       -       -       -       -       -       DctM\nk141_23643      272563.CD630_25090      3.3e-45 159.0   COG1486@1|root,COG1486@2|Bacteria,1TQ9I@1239|Firmicutes,24995@186801|Clostridia 186801|Clostridia       G       family 4        -       -       3.2.1.122,3.2.1.86 ko:K01222,ko:K01232     ko00010,ko00500,map00010,map00500       -       R00837,R00838,R00839,R05133,R05134,R06113       RC00049,RC00171,RC00714 ko00000,ko00001,ko01000 -       GH4,GT4 -       Glyco_hydro_4,Glyco_hydro_4C\nk141_70929      1122985.HMPREF1991_02897        3.54e-11        65.5    COG1595@1|root,COG1595@2|Bacteria,4NS12@976|Bacteroidetes,2FQ76@200643|Bacteroidia      976|Bacteroidetes       K       RNA polymerase sigma-70 factor     -       -       -       ko:K03088       -       -       -       -       ko00000,ko03021 -       -       -       Sigma70_r2,Sigma70_r4_2\nBastante informação! Mas essencialmente semelhante à ferramenta anterior: Temos em cada linha uma sequência de busca advinda do nosso arquivo de montagem, e em cada uma das outras colunas, anotações para diferentes bancos de dados de informação funcional, além de colunas detalhando a qualidade de alinhamnto das nossas sequências à esses identificadores.\nPara mais informações sobre o arquivo de saída do eggNOG-mapper, confira a documentação da ferramenta.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Anotação Funcional</span>"
    ]
  },
  {
    "objectID": "content/06_analise_downstream.html",
    "href": "content/06_analise_downstream.html",
    "title": "6  Análises Downstream",
    "section": "",
    "text": "6.1 Cálculos de Diversidade",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Análises Downstream</span>"
    ]
  },
  {
    "objectID": "content/06_analise_downstream.html#extraindo-informação-funcional",
    "href": "content/06_analise_downstream.html#extraindo-informação-funcional",
    "title": "6  Análises Downstream",
    "section": "6.2 Extraindo informação funcional",
    "text": "6.2 Extraindo informação funcional",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Análises Downstream</span>"
    ]
  },
  {
    "objectID": "content/04_taxonomia.html#contexto",
    "href": "content/04_taxonomia.html#contexto",
    "title": "4  Classificação Taxonômica",
    "section": "",
    "text": "Ye et al, 2019",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classificação Taxonômica</span>"
    ]
  },
  {
    "objectID": "content/04_taxonomia.html#adquirindo-o-banco-de-dados",
    "href": "content/04_taxonomia.html#adquirindo-o-banco-de-dados",
    "title": "4  Classificação Taxonômica",
    "section": "4.2 Adquirindo o banco de dados",
    "text": "4.2 Adquirindo o banco de dados\nMude o diretório atual para \"Pipeline/alignment/db\" (pois esse banco também será usado na seção Seção 5.1.1, do alinhamento) e baixe o banco de dados de proteínas do NCBI-nr:\n\n\n\n\n\n\nAviso\n\n\n\nO banco de dados é muito grande e inviável de se instalar em um computador pessoal. Use um cluster de alta performance ou ambientes similares!\n\n\n$ wget ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nr.gz  \n$ pigz -d nr.gz -p 8  \nConstrua um índice DIAMOND:\n$ diamond makedb --in nr -d ../index/nr  \nMude o diretório atual para “Pipeline/taxonomic/db” e baixe os seguintes arquivos:\nwget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz\nwget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.gz\nEsses são os bancos de dados que utilizaremos para executar o Kaiju\nExtraia e descompacte os arquivos necessários:\ntar -xf taxdump.tar.gz nodes.dmp names.dmp\npigz -d prot.accession2taxid.gz -p 8\nConstrua um índice Kaiju:\nkaiju-convertNR -t nodes.dmp -g prot.accession2taxid -e ~/miniconda3/envs/medusaPipeline/bin/kaiju-excluded-accessions.txt -a -o kaijuNR.fasta -i ../../alignment/db/nr\nkaiju-mkbwt -n 8 -a ACDEFGHIKLMNPQRSTVWY -o kaijuNR kaijuNR.fasta\nkaiju-mkfmi kaijuNR\n\n\n\n\n\n\nNota\n\n\n\nNa chamada kaiju-convertNR, por padrão, são incluídas apenas sequências de Archaea, Bactérias e Vírus do NCBI-nr. Este comportamento pode ser alterado com o argumento -l, passando um arquivo de entrada como ~/miniconda3/envs/medusaPipeline/bin/kaiju-taxonlistEuk.tsv. Este argumento utiliza apenas sequências com ancestrais listados no arquivo.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Classificação Taxonômica</span>"
    ]
  }
]